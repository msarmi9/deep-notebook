{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\">Objective</a></span></li><li><span><a href=\"#The-Chain-Rule\" data-toc-modified-id=\"The-Chain-Rule-2\">The Chain Rule</a></span></li><li><span><a href=\"#Activations\" data-toc-modified-id=\"Activations-3\">Activations</a></span><ul class=\"toc-item\"><li><span><a href=\"#ReLU\" data-toc-modified-id=\"ReLU-3.1\">ReLU</a></span></li><li><span><a href=\"#Softmax\" data-toc-modified-id=\"Softmax-3.2\">Softmax</a></span></li></ul></li><li><span><a href=\"#Cross-Entropy-Loss\" data-toc-modified-id=\"Cross-Entropy-Loss-4\">Cross Entropy Loss</a></span></li><li><span><a href=\"#Linear-Layers\" data-toc-modified-id=\"Linear-Layers-5\">Linear Layers</a></span></li><li><span><a href=\"#Putting-It-All-Together\" data-toc-modified-id=\"Putting-It-All-Together-6\">Putting It All Together</a></span><ul class=\"toc-item\"><li><span><a href=\"#Trainer\" data-toc-modified-id=\"Trainer-6.1\">Trainer</a></span></li></ul></li><li><span><a href=\"#Training-Data\" data-toc-modified-id=\"Training-Data-7\">Training Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-process-Data\" data-toc-modified-id=\"Pre-process-Data-7.1\">Pre-process Data</a></span></li><li><span><a href=\"#Datasets\" data-toc-modified-id=\"Datasets-7.2\">Datasets</a></span></li></ul></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-8\">Train</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:02:42.878702Z",
     "start_time": "2020-07-07T18:02:42.207771Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:02:42.884519Z",
     "start_time": "2020-07-07T18:02:42.881353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The goal of this notebook is to write a simple two-layer feedforward neural net in python using just numpy. It's very much inspired by [this](https://sgugger.github.io/a-simple-neural-net-in-numpy.html#a-simple-neural-net-in-numpy) fantastic blog by Sylvain Gugger. We won't pretend that this notebook will add upon or improve Sylvain's post. We just want to explain things in our own way to help us understand things a little bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the thing that really makes networks tick--back propagation. We like to think of it as just the chain rule, since that's really all it is. The goal of back propagation is to nudge the weights of a network in whichever direction the loss is decreasing the quickest. This means we need to know the gradient of the loss function.\n",
    "\n",
    "The loss function takes the network's final output and our labels and returns a single number. For classification, the outputs are typically the probabilities (or log-probabilities) of each class, so have shape $(n_{obsv}, \\, n_{classes})$. Those probabilities are the output of the final activation layer (e.g. log-softmax), which are themselves the output of the final linear layer, which are themselves--just kidding! We think we've got the picture.\n",
    "\n",
    "The network's final output is just a composition of all the layers of the network--linear and activation--which means computing the gradient of the loss with respect to each set of linear weights will involve lots and lots of chain rule. To help us write things in a nicer way, we'll denote the $i$-th layer by $f_i$, so that a network with depth $d$ can be expressed as the composition of $d$ functions:\n",
    "\n",
    "$$x \\quad \\to \\quad f_d \\circ f_{d-1} \\circ \\dots \\circ f_1(x)$$\n",
    "\n",
    "Now, although the loss function is not really a layer, it'll make our lives easier if we let $f_d$ be the loss function. It'll also help if we tighten the notation up a bit and let $x_i$ denote the output of the $i$-th layer:\n",
    "\n",
    "\\begin{align}\n",
    "    x_1 &= f_1(x) \\\\\n",
    "    x_2 &= f_2(x_1) = f_2(f_1(x)) \\\\\n",
    "    &\\vdots \\\\\n",
    "    x_d &= f_d(x_{d-1}) = f_d \\circ f_{d-1} \\circ \\dots \\circ f_1(x) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Now we can express the gradient of the loss with respect to the network's final output $x_{d-1}$ as\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial x_{d-1}} = \\frac{\\partial f_d}{\\partial x_{d-1}}(x_{d-1}) $$\n",
    "\n",
    "For the gradient with respect to the output $x_{d-2}$ of the network's penultimate layer, we can use the fact that $x_{d-1} = f_{d-1}(x_{d-2})$ to write \n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial x_{d-2}} = \\frac{\\partial f_d}{\\partial x_{d-1}} \\times \\frac{\\partial f_{d-1}}{\\partial x_{d-2}}(x_{d-2})$$\n",
    "\n",
    "Note that computing the gradient with respect to the output of the network's second to last year involves the gradient with respect to the output of the final layer. This means we first have to compute the gradient of the final layer and then pass that gradient backwards through the network to compute the next gradient. This is why it's called \"back propagation\" and not \"just the chain rule\". Envisioning the gradients being passed backwards like a baton is helpful when trying to understand how we update a network's weights.\n",
    "\n",
    "In our discussion so far we just considered a single variable $x$ as the input to our network. The discussion is a bit more complex when considering multiple variables $x_1, \\dots, x_m$ of input but the gist is the same (it involves the Jacobian--although we'll pretend we didn't hear that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations\n",
    "\n",
    "If we let $f$ be an activation function and $y = f(x)$ its output given the output $x$ of the previous layer, we can express the gradient as\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial f} = \\frac{\\partial \\text{loss}}{\\partial y} \\times \\frac{\\partial f}{\\partial x} = \\frac{\\partial \\text{loss}}{\\partial y} \\times f'(x)$$ \n",
    "\n",
    "Remember that the gradient $\\text{grad}$ of the loss with respect to $y$ will have already been computed and passed backwards when it's time to compute the gradient with respect to $x$. This means that all we need to do is multiply $\\text{grad}$ by $f'(x)$ element-wise. Nice, right? Note that we'll need to cache $x$ for computing the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our activation functions except the last will be ReLUs:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{ReLU}(x) = \\text{max}(x, 0) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "You might recognize its derivative as the Heavside function,\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:02:42.890552Z",
     "start_time": "2020-07-07T18:02:42.886864Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"Container for computing the forward and backward pass of ReLU.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through ReLU.\"\"\"\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Return the gradient where x is positive, otherwise zero.\"\"\"\n",
    "        return np.where(self.x > 0, grad, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Our final activation will be softmax. Given a single observation $x = [x_1, \\dots, x_m]$ as input, softmax returns $y = [y_1, \\dots, y_m]$, where\n",
    "\n",
    "$$ y_i = \\text{softmax}(x_i) = \\frac{e^{x_i}}{e^{x_1} + \\dots + e^{x_m}} $$\n",
    "\n",
    "If you use the quotient rule and treat the partials with respect to $x_i$ and $x_j$ separately, you'll find \n",
    "\n",
    "$$\\frac{\\partial y_i}{\\partial x_j} = \\begin{cases} y_i(1 - y_i) & j = i \\\\ -y_i y_j & j \\neq i \\end{cases}$$\n",
    "\n",
    "Since each of the $y_k$ depend on all of the $x_j$, when we compute the gradient of the loss with respect to $x_j$ every $y_k$ will contribute. Sylvain walks us through the calculation step-by-step and in the end we get\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial x_j} =  y_j \\left(g_j - \\sum_{k=1}^m g_k y_k \\right)$$\n",
    "\n",
    "where $g_k$ denotes the gradient of the loss with respect to $y_k$. Note that just like with ReLU, we'll need to cache $y$ for computing the backward pass. \n",
    "\n",
    "Before we go any further though, let's do a sanity check on the dimensions involved. The above derivation handles a single observation $x$. If instead we're given a mini-batch of dimension $\\text{bs} \\times m$, then both $y$ and $g$ also have shape $\\text{bs} \\times m$. To compute the summation term for all observations in one go, we'll multiply $y$ and $g$ element-wise and then take the sum across the rows (`axis=1`), producing a column vector of shape $\\text{bs} \\times 1$. Each element of the resulting column vector will be subtracted from corresponding row of $g$ via broadcasting. The final output will have shape $\\text{bs} \\times m$.\n",
    "\n",
    "\n",
    "Alrighty. Now it's time to translate the above to code. We'll implement a numerically stable version of softmax that avoids overflow, which you can read more about [here](https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/). Sadly, numpy doesn't have an `unsqueeze()` method like pytorch, so we'll need to call `reshape(-1, 1)` or `[:, None]` to periodically to turn row vectors into column vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.775046Z",
     "start_time": "2020-07-07T18:26:24.767210Z"
    }
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"Container for computing the forward and backward pass of softmax.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Return softmax predictions on a mini-batch.\"\"\"\n",
    "#         x = x - x.max(axis=1, keepdims=True)\n",
    "        self.y = np.exp(x) / np.exp(x).sum(axis=1, keepdims=True)\n",
    "        return self.y\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Return the gradient given the gradient of the loss w.r.t softmax predictions.\"\"\"\n",
    "        return self.y * (grad - (grad * self.y).sum(axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.786734Z",
     "start_time": "2020-07-07T18:26:24.778884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26894142, 0.73105858],\n",
       "       [0.5       , 0.5       ]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's walk through the backward pass step-by-step\n",
    "X = np.array([[1, 2], [1, 1]])\n",
    "grad = np.array([[1, -1], [1, 1]])\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax.forward(X)  # y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.795214Z",
     "start_time": "2020-07-07T18:26:24.789908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.46211716],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply San Francisco``grad`` and ``y`` element-wise, then sum across rows\n",
    "(grad * softmax.y).sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.801952Z",
     "start_time": "2020-07-07T18:26:24.798095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.46211716, -0.53788284],\n",
       "       [ 0.        ,  0.        ]])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract from ``grad`` via broadcasting\n",
    "grad - (grad * softmax.y).sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.807294Z",
     "start_time": "2020-07-07T18:26:24.803440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39322387, -0.39322387],\n",
       "       [ 0.        ,  0.        ]])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last, multiply with ``y`` element-wise\n",
    "softmax.y * (grad - (grad * softmax.y).sum(axis=1).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss\n",
    "\n",
    "Cross entropy takes as input the softmax probabilities $\\hat{y}$ and labels $y$ and returns a single number. Each softmax probability $\\hat{y}_k$ contributes $-\\ln \\hat{y}_k$ to the loss whenever the true label is $y_k$. Since $y$ consists of one-hot encoded labels, only one $y_k$ is equal to one and the rest are zero.\n",
    "\n",
    "$$\\text{CE}(y, \\hat{y}) = - \\sum_{k=1} y_k \\ln \\hat{y}_k  $$\n",
    "\n",
    "Since the loss function is the \"last layer\", there's no chain rule involved. The gradient is simply\n",
    "\n",
    "$$\\frac{\\partial \\text{CE}}{\\partial y_k} = -\\frac{y_k}{\\hat{y}_k}$$\n",
    "\n",
    "If you're like me and worry about division by zero errors, you'll be concerned about the softmax layer returning probabilities close to zero. Fret not. Sylvain says that in practice the probabilities are clipped to $10^{-8}$ (usually). This time we'll need to cache both the labels $y_k$ and probabilities $\\hat{y}_k$ for computing the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.812871Z",
     "start_time": "2020-07-07T18:26:24.808753Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    \"\"\"Container for computing the forward and backward pass of cross entropy.\"\"\"\n",
    "    \n",
    "    def forward(self, y, y_hat):\n",
    "        \"\"\"Return the cross-entropy given labels and soft predictions.\"\"\"\n",
    "        self.y, self.y_hat = y, y_hat.clip(min=1e-8, max=1-1e-8)\n",
    "        return np.where(y==1, -np.log(self.y_hat), 0).sum(axis=1)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Return the gradient of cross entropy w.r.t soft predictions.\"\"\"\n",
    "        return np.where(self.y==1, -1 / self.y_hat, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layers\n",
    "\n",
    "The last part of a network we need to model are the linear layers, which consist of weights and biases:\n",
    "\n",
    "$$Y = XW + b$$\n",
    "\n",
    "where $X$ is mini-batch of shape $bs \\times n_{inp}$ and the weight matrix $W$ has shape $n_{inp} \\times n_{out}$. The bias is a column vector of shape $n_{out} \\times 1$ and is added to the product $XW$ via broadcasting. The final output has shape $bs \\times n_{out}$.\n",
    "\n",
    "The output $y_i$ given a single input vector $x = [x_1, \\dots, x_{n_{inp}}]$ is just the dot product of the $x$ with the $i$-th column of $W$ plus the $i$-th bias term:\n",
    "\n",
    "$$y_i = \\sum_{k=1}^{n_{inp}} x_k w_{ki} + b_i$$.\n",
    "\n",
    "The gradients with respect to $w_{ki}$ and $b_i$ are\n",
    "\n",
    "$$ \\frac{\\partial \\text{loss}}{\\partial w_{ki}} = \\frac{\\partial \\text{loss}}{\\partial y_i} \\times \\frac{\\partial y_j}{\\partial w_{ki}} = x_k \\frac{\\partial \\text{loss}}{\\partial y_i} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\frac{\\partial \\text{loss}}{\\partial b_i} = \\frac{\\partial \\text{loss}}{\\partial y_i} \\times \\frac{\\partial y_i}{\\partial b_i} = \\frac{\\partial \\text{loss}}{\\partial y_i} $$\n",
    "\n",
    "Let's make this a little more concrete by considering what happens when we have a batch size of 1. As before, let $\\text{grad}$ denote the gradients of the loss with respect to the $y_i$. Suppose $n_{inp}=3$ and $n_{out}=2$ so that\n",
    "\n",
    "\\begin{align}\n",
    "    x &= [x_1, x_2, x_3] \\\\\n",
    "    \\text{grad} &= [\\text{grad}_1, \\text{grad}_2]\n",
    "\\end{align}\n",
    "\n",
    "Then $W$ has shape $(3, 2)$ and the matrix of gradient updates with respect to the weights looks like\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    x_1 \\text{grad}_1 & x_1 \\text{grad}_2 \\\\\n",
    "    x_2 \\text{grad}_1 & x_2 \\text{grad}_2 \\\\\n",
    "    x_3 \\text{grad}_1 & x_3 \\text{grad}_2\n",
    "\\end{bmatrix}\n",
    "\n",
    "When we have a batch size greater than 1, $X$ has shape $(bs, n_{inp})$ and $\\text{grad}$ has shape $(bs, n_{out})$. In order to multiply them to get a matrix of shape $(n_{inp}, n_{out})$ we need to resize $X$ to have shape $(bs, n_{inp}, 1)$ and $\\text{grad}$ to have shape $(bs, 1, n_{out})$. This way, ordinary matrix multiplication over the last two dimensions gives the same result as above for the entire mini-batch, yielding a tensor of shape $(bs, n_{inp}, n_{out})$. The last thing is to average the gradients over the batch dimension, producing a final update matrix of size $(n_{inp}, n_{out})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients of the loss with respect to the weights and bias tell us how to update them on each backward pass, but we still need to compute the gradient with respect to the inputs $x_k$, which will be passed backwards to the previous layer:\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial x_k} = \\sum_{k=1}^{n_{inp}} \\frac{\\partial \\text{loss}}{\\partial y_k} \\times \\frac{\\partial y_k}{\\partial x_k} = \\sum_{k=1}^{n_{inp}} \\frac{\\partial \\text{loss}}{\\partial y_k} w_{ki}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sylvain tells us that we can write this as \n",
    "\n",
    "$$ \\text{new grad} = \\text{old grad} \\times W^t$$\n",
    "\n",
    "and we believe him because the shapes work out: $\\text{old grad}$ has shape $(bs, n_{out})$ and $W$ has shape $(n_{inp}, n_{out})$, meaning $\\text{new grad}$ has shape $(bs, n_{inp})$, just as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.819547Z",
     "start_time": "2020-07-07T18:26:24.814361Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Container for a linear layer holding weights and biases.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inp, n_out):\n",
    "        \"\"\"Initialise weights and biases.\"\"\"\n",
    "        self.W = np.random.uniform(-1 / np.sqrt(n_inp), 1 / np.sqrt(n_inp), (n_inp, n_out))\n",
    "        self.b = np.zeros(n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through the layer.\"\"\"\n",
    "        self.x = x\n",
    "        return x @ self.W + self.b\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Backpropagate the gradient to the preceding layer.\"\"\"\n",
    "        self.grad_W = (self.x[:,:,None] @ grad[:,None,:]).mean(axis=0)\n",
    "        self.grad_b = grad.mean(axis=0)\n",
    "        return grad @ self.W.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "\n",
    "We finally have all the pieces in place to build our own network. Taking after $nn.Sequential$ we'll design it to take an arbitrary collection of layers and an arbitrary loss criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.827491Z",
     "start_time": "2020-07-07T18:26:24.822700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"Container for a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, criterion, metric):\n",
    "        \"\"\"Initialise layers, loss criterion, and evaluation metric.\"\"\"\n",
    "        self.layers = layers\n",
    "        self.criterion = criterion\n",
    "        self.metric = metric\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through the network.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate gradients to the start of the network.\"\"\"\n",
    "        grad = self.criterion.backward()\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.833930Z",
     "start_time": "2020-07-07T18:26:24.830314Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Container for updating a model's weights via SGD.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "                  \n",
    "    def step(self):\n",
    "        \"\"\"Update the weights and biases of all linear layers.\"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.W -= self.lr * layer.grad_W\n",
    "                layer.b -= self.lr * layer.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "For convenience, we'll wrap all of the training functionality we need into a `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.838414Z",
     "start_time": "2020-07-07T18:26:24.835467Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    \"\"\"Compute accuracy of a mini-batch given labels and soft predictions.\"\"\"\n",
    "    y_pred = np.argmax(y_hat, axis=1)\n",
    "    return (y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.848925Z",
     "start_time": "2020-07-07T18:26:24.840536Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Container for training a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, train_dl, val_dl):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"Train for a single epoch and return the loss.\"\"\"\n",
    "        loss, n = 0, 0\n",
    "        for x, y in self.train_dl:\n",
    "            y_hat = self.model.forward(x)\n",
    "            batch_loss = self.model.criterion.forward(y, y_hat).sum()\n",
    "            self.model.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n\n",
    "            \n",
    "    def train(self, n_epochs, log_level=1):\n",
    "        \"\"\"Train for multiple epochs.\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self._train()\n",
    "            self.optimizer.step()\n",
    "            val_loss, val_metric = self.evaluate(self.val_dl)\n",
    "            if (epoch + 1) % log_level == 0:\n",
    "                print(f\"{epoch= :2d} | {loss= :.3f} | {val_loss= :.3f} | {val_metric= :.3f}\")\n",
    "    \n",
    "    def evaluate(self, dl):\n",
    "        \"\"\"Return loss and metric on validation or test set.\"\"\"\n",
    "        loss, n, metric = 0, 0, 0\n",
    "        for x, y in dl:\n",
    "            y_hat = self.model.forward(x)\n",
    "            batch_loss = self.model.criterion.forward(y, y_hat).sum()\n",
    "            batch_metric = self.model.metric(y, y_hat)\n",
    "            metric += len(y) * batch_metric\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n, metric / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.863991Z",
     "start_time": "2020-07-07T18:26:24.850371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First up is breast cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.869955Z",
     "start_time": "2020-07-07T18:26:24.865632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test-split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.875281Z",
     "start_time": "2020-07-07T18:26:24.871442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalise\n",
    "mu, sigma = X_train.mean(), X_train.std()\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_val = (X_val - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.881177Z",
     "start_time": "2020-07-07T18:26:24.876845Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Container for returning inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1, 1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __setitem__(self, idx, val):\n",
    "        self.X[idx], self.y[idx] = val\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.887676Z",
     "start_time": "2020-07-07T18:26:24.882559Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Container for returning a mini-batch of inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, ds, batch_size, shuffle=False):\n",
    "        self.ds = ds\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        \"\"\"Shuffle inputs and targets.\"\"\"\n",
    "        idxs = np.random.permutation(len(self.ds))\n",
    "        self.ds = Dataset(*self.ds[idxs])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a mini-batch of inputs and targets.\"\"\"\n",
    "        if self.shuffle: self.shuffle_data()\n",
    "        n_batches = len(self.ds) // self.batch_size\n",
    "        for i in range(n_batches):\n",
    "            yield self.ds[i * self.batch_size: (i + 1) * self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.893037Z",
     "start_time": "2020-07-07T18:26:24.889287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_ds = Dataset(X_train, y_train)\n",
    "val_ds = Dataset(X_val, y_val)\n",
    "\n",
    "batch_size=50\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Let's take this thing for a test drive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.898515Z",
     "start_time": "2020-07-07T18:26:24.894705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# Initialise model with layers, criterion, and metric\n",
    "metric = accuracy\n",
    "criterion = CrossEntropy()\n",
    "layers = [Linear(n_inp, n_classes), Softmax()]\n",
    "model = Sequential(layers, criterion, metric)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.1)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T18:26:24.935190Z",
     "start_time": "2020-07-07T18:26:24.900051Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.878 | val_loss= 0.873 | val_metric= 0.568\n",
      "epoch=  1 | loss= 0.863 | val_loss= 0.873 | val_metric= 0.567\n",
      "epoch=  2 | loss= 0.860 | val_loss= 0.873 | val_metric= 0.563\n",
      "epoch=  3 | loss= 0.872 | val_loss= 0.873 | val_metric= 0.541\n",
      "epoch=  4 | loss= 0.860 | val_loss= 0.873 | val_metric= 0.507\n",
      "epoch=  5 | loss= 0.869 | val_loss= 0.873 | val_metric= 0.546\n",
      "epoch=  6 | loss= 0.860 | val_loss= 0.873 | val_metric= 0.546\n",
      "epoch=  7 | loss= 0.863 | val_loss= 0.873 | val_metric= 0.514\n",
      "epoch=  8 | loss= 0.863 | val_loss= 0.873 | val_metric= 0.504\n",
      "epoch=  9 | loss= 0.866 | val_loss= 0.873 | val_metric= 0.544\n",
      "epoch= 10 | loss= 0.856 | val_loss= 0.873 | val_metric= 0.520\n",
      "epoch= 11 | loss= 0.863 | val_loss= 0.873 | val_metric= 0.536\n",
      "epoch= 12 | loss= 0.866 | val_loss= 0.873 | val_metric= 0.555\n",
      "epoch= 13 | loss= 0.863 | val_loss= 0.873 | val_metric= 0.549\n",
      "epoch= 14 | loss= 0.860 | val_loss= 0.873 | val_metric= 0.504\n",
      "epoch= 15 | loss= 0.866 | val_loss= 0.873 | val_metric= 0.524\n",
      "epoch= 16 | loss= 0.863 | val_loss= 0.873 | val_metric= 0.541\n",
      "epoch= 17 | loss= 0.869 | val_loss= 0.873 | val_metric= 0.549\n",
      "epoch= 18 | loss= 0.856 | val_loss= 0.873 | val_metric= 0.494\n",
      "epoch= 19 | loss= 0.860 | val_loss= 0.873 | val_metric= 0.520\n"
     ]
    }
   ],
   "source": [
    "trainer.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
