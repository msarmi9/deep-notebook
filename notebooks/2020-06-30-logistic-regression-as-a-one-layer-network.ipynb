{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\">Objective</a></span></li><li><span><a href=\"#The-Chain-Rule\" data-toc-modified-id=\"The-Chain-Rule-2\">The Chain Rule</a></span></li><li><span><a href=\"#(Binary)-Cross-Entropy-Loss\" data-toc-modified-id=\"(Binary)-Cross-Entropy-Loss-3\">(Binary) Cross Entropy Loss</a></span></li><li><span><a href=\"#Activations\" data-toc-modified-id=\"Activations-4\">Activations</a></span></li><li><span><a href=\"#Linear-Layer\" data-toc-modified-id=\"Linear-Layer-5\">Linear Layer</a></span></li><li><span><a href=\"#Putting-It-All-Together\" data-toc-modified-id=\"Putting-It-All-Together-6\">Putting It All Together</a></span></li><li><span><a href=\"#Our-Evaluation-Metric\" data-toc-modified-id=\"Our-Evaluation-Metric-7\">Our Evaluation Metric</a></span></li><li><span><a href=\"#Trainer\" data-toc-modified-id=\"Trainer-8\">Trainer</a></span></li><li><span><a href=\"#Pre-process-Data\" data-toc-modified-id=\"Pre-process-Data-9\">Pre-process Data</a></span></li><li><span><a href=\"#Datasets\" data-toc-modified-id=\"Datasets-10\">Datasets</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-11\">Train</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:17:09.276037Z",
     "start_time": "2020-07-04T03:17:08.700410Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:08:42.339252Z",
     "start_time": "2020-07-04T03:08:42.335282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "The goal of this notebook is to build a logistic regression model as a neural network. The network will consist of a single linear layer followed be a sigmoid activation and binary cross entropy as the loss function. When we're done we'll see that we're able to identify malignant tumors on sklearn's breast cancer dataset with 90% accuracy in just 10 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chain Rule\n",
    "\n",
    "Before we build the network, we need to understand how to update the network's weights. We can view our network as a composition of three functions\n",
    "\n",
    "$$x \\to \\text{BCE} \\circ \\text{Sigmoid} \\circ \\text{Linear}(x)$$\n",
    "\n",
    "While the loss function is not usually viewed as a layer of the network, treating it as the final layer makes computing the gradients simpler. Let's denote the output of the $i$-th layer by $x_i$ so that\n",
    "\n",
    "\\begin{align}\n",
    "    x_1 &= \\text{Linear}(x)     \\\\\n",
    "    x_2 &= \\text{Sigmoid}(x_1)  \\\\\n",
    "    x_3 &= \\text{BCE}(x_2)\n",
    "\\end{align}\n",
    "\n",
    "Since the last output we have is $x_3 = \\text{BCE}(x_2)$, the first gradient to compute is the gradient of $\\text{BCE}$ with respect to $x_2$\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_2} = \\frac{\\partial \\text{BCE}}{\\partial x_2}(x_2) $$\n",
    "\n",
    "Next we have $x_2 = \\text{Sigmoid}(x_1)$, so the chain rule gives us\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_1} = \\frac{\\partial \\text{BCE}}{\\partial x_2} \\times \\frac{\\partial \\text{Sigmoid}}{\\partial x_1}(x_1)$$\n",
    "\n",
    "Last we have $x_1 = \\text{Linear}(x)$, so the final gradient is\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x} = \\frac{\\partial \\text{BCE}}{\\partial x_1} \\times \\frac{\\partial \\text{Linear}}{\\partial x_1}(x)$$\n",
    "\n",
    "Notice something? The first gradient we compute--the gradient with respect to the Sigmoid output--is used to compute the next gradient--the gradient with respect to the Linear output. To compute all of the gradients, we need to start at the last layer and successively pass back the gradient to the previous layer. That's why it's called backpropagation and not \"just the chain rule\". It really is helpful to envision passing the gradients backwards through the network like a baton.\n",
    "\n",
    "As a final note, so far we've been treating the input as a single variable, but most of the time the input will have more than one dimension. Don't worry, computing the gradients in the multi-variate case is more or less the same (it involves something called the Jacobian--but we'll pretend we didn't hear that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Binary) Cross Entropy Loss\n",
    "\n",
    "The only difference between binary cross entropy and cross entropy is that cross entropy requires an output probability for every single class, whereas binary cross entropy requires just a single output probability--the probability of the positive class. This is slightly annoying, because it means their derivatives have different functional forms.\n",
    "\n",
    "Binary cross entropy penalizes predictions by the logarithm of their probability $\\hat{y}$ (which we called $x_2$ above):\n",
    "\n",
    "$$\\text{BCE}(y, \\hat{y}) = -y \\ln(\\hat{y}) + (1 - y)\\ln(1 - \\hat{y})$$\n",
    "\n",
    "After simplifying, you'll find its derivative is\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1 - y)}$$\n",
    "\n",
    "Notice that we'll need to cache the labels and predictions to compute the backward pass. Since this is the first gradient we compute, there's no previous gradient to take in. However, the next gradient (belonging to the Sigmoid layer) will receive the above BCE gradient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:08:42.348463Z",
     "start_time": "2020-07-04T03:08:42.342903Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    \"\"\"Container for the forward and backward pass of BCE.\"\"\"\n",
    "    \n",
    "    def forward(self, y, y_hat):\n",
    "        \"\"\"Return binary cross entropy given targets and predictions.\"\"\"\n",
    "        self.y, self.y_hat, = y, y_hat\n",
    "        return np.where(y==1, -np.log(y_hat), -np.log(1 - y_hat)).mean()\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate the gradient with respect to soft predictions.\"\"\"\n",
    "        return (self.y_hat - y) / (self.y_hat * (1 - self.y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations\n",
    "\n",
    "The easiest components to handle are the activation functions. Our activation is Sigmoid, which you'll often see defined as one of \n",
    "\n",
    "$$\\text{Sigmoid}(x) = \\frac{1}{1 + \\text{exp}(-x)} \\quad \\text{or} \\quad \\frac{\\text{exp}(x)}{1 + \\text{exp}(x)}$$\n",
    "\n",
    "It turns out that we need both versions to implement a numerically stable version of Sigmoid. Notice how when $x$ is very negative, $\\text{exp}(-x)$ is incredibly large, and when $x$ is very positive $\\text{exp}(x)$ is incredibly large--in both cases, too large to store in memory. The easy fix is to use the first version when $x > 0$ and the second when $x < 0$.\n",
    "\n",
    "After simplifying, you'll find the derivative of $\\text{Sigmoid}$ is\n",
    "\n",
    "$$\\frac{\\partial \\text{Sigmoid}}{\\partial x} = \\text{Sigmoid}(x)(1 - \\text{Sigmoid}(x))$$\n",
    "\n",
    "You may have noticed something peculiar when comparing this derivative to that of BCE. Since we denoted $\\hat{y}$ to be the output of the Sigmoid layer, the derivative of sigmoid is exactly the same as the denominator in the derivative of BCE, so the two terms will cancel when multiplied (which is exactly what the chain rule tells us will happen). This is the reason you see functions like PyTorch's `binary_cross_entropy_with_logits`, which skips the Sigmoid activation and computes BCE directly from the inputs to the Sigmoid layer (which people call logits). It's more efficient computationally to do both in one go. However, since we're just trying to get our hands dirty to understand how networks work, we won't worry about optimizing things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:08:42.355936Z",
     "start_time": "2020-07-04T03:08:42.351487Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: numerically stable sigmoid\n",
    "class Sigmoid:\n",
    "    \"\"\"Container for the forward and backward pass of sigmoid.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a sigmoid layer.\"\"\"\n",
    "        return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        \n",
    "    def backward(self, x, grad):\n",
    "        \"\"\"Backpropagate the gradient given the previous gradient.\"\"\"\n",
    "        return grad * self.forward(x) * (1 - self.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer\n",
    "\n",
    "The last component we need to implement is the linear layer, which contains weights and biases. Denoting the output of this layer by $z_i$, we have\n",
    "\n",
    "$$z_i = \\text{Linear}(x) = xw + b$$\n",
    "\n",
    "If $x$ is a mini-batch of shape $(bs, n_{inp})$ then $w$ has shape $(n_{inp}, 1)$ and $b$ has shape $(1,)$, with addition being done via broadcasting. When computing the gradient, we'll image that we just have a batch size of one \n",
    "\n",
    "$$x = [x_1, \\dots, x_{n_{inp}}]$$ \n",
    "\n",
    "There are two gradients to compute this time around, one with respect to the weights and the other with respect to the bias. To make life easier, let's write things out in terms of coordinates:\n",
    "\n",
    "$$z_i = \\sum_{k=1}^{n_{inp}} x_k w_{ki} + b$$\n",
    "\n",
    "Then we get\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial w_{ki}} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}\n",
    "{\\partial w_{ki}} = x_k \\frac{\\partial \\text{BCE}}{\\partial z_i}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial b} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}{\\partial b} = \\frac{\\partial \\text{BCE}}{\\partial z_i}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we'll already have the gradient of the loss with respect to the output $z_i$ of the linear layer stored in a variable $\\text{grad}$ when it's time to compute the gradients with respect to the weights and biases. The nice thing is that $\\text{grad}$ is exactly the gradient with respect to the $b$, so we only need to figure out how to write the gradient with respect to $w$ as a matrix product.\n",
    "\n",
    "Whenever I have a hard time doing something like this, I just focus on getting the shapes right:\n",
    "\n",
    "* $x$ has shape $(bs, n_{inp})$ \n",
    "* $\\text{grad}$ has shape $(bs, 1)$\n",
    "* $\\text{grad}_w$ has shape $(n_{inp}, 1)$\n",
    "\n",
    "The only way we can multiply $x$ and $\\text{grad}$ and get something of shape $(n_{inp}, 1)$ is to resize $x$ to have shape $(bs, n_{inp}, 1)$ and $\\text{grad}$ to have shape $(bs, 1, 1)$ so that ordinary matrix multiplication over the last two dimensions gives the shape $(n_{inp}, 1)$. The last thing to remember is that we average the gradients over the batch dimension to produce the final gradient updates.\n",
    "\n",
    "Note that were there another linear layer we would also need to compute the gradient of the loss with respect to the inputs $x$ so that we could keep backpropagating the gradient, however, this isn't anymore complicated than what we've done so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:18:57.793404Z",
     "start_time": "2020-07-04T03:18:57.783996Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Container for the forward and backward pass of a linear layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inp, n_out):\n",
    "        self.weights = np.random.uniform(-1 / np.sqrt(n_inp), 1 / np.sqrt(n_inp), (n_inp, n_out))\n",
    "        self.bias = np.zeros(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a linear layer.\"\"\"\n",
    "        self.x = x\n",
    "        return x @ self.weights + self.bias\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Compute the gradients of the weights and biases given previous gradient.\"\"\"\n",
    "        self.grad_w = (self.x[:,:,None] @ grad[:,None,:]).mean(axis=0)\n",
    "        self.grad_b = grad.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "\n",
    "Wohoo! It's finally time to string together all the work we've done so far into a single network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:22:20.234362Z",
     "start_time": "2020-07-04T03:22:20.226925Z"
    }
   },
   "outputs": [],
   "source": [
    "class OneLayerBinaryClassifier:\n",
    "    \"\"\"Container for a single layer binary classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inp):\n",
    "        \"\"\"Initialise layers and loss criterion.\"\"\"\n",
    "        self.linear = Linear(n_inp, 1)\n",
    "        self.out = Sigmoid()\n",
    "        self.criterion = BinaryCrossEntropy()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through the network.\"\"\"\n",
    "        return self.out.forward(self.linear.forward(x))\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate gradients through the network.\"\"\"\n",
    "        grad = self.criterion.backward()\n",
    "        self.linear.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:23:05.727881Z",
     "start_time": "2020-07-04T03:23:05.721982Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Container for optimizing a model via SGD.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        \"\"\"Initialise model and learning rate.\"\"\"\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"Update weights and biases of linear layers.\"\"\"\n",
    "        #self.model.linear.weights -= self.lr * self.model.linear.grad_w\n",
    "        self.model.linear.bias -= self.lr * self.model.linear.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Evaluation Metric\n",
    "\n",
    "For simplicity, we'll just consider accuracy for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:14:16.991033Z",
     "start_time": "2020-07-04T03:14:16.986568Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    \"\"\"Compute accuracy given soft binary predictions.\"\"\"\n",
    "    y_pred = y_hat > 0.5\n",
    "    return (y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "To make life easier, let's wrap all of the functionality we need to train a network into a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:21:00.264853Z",
     "start_time": "2020-07-04T03:21:00.254652Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Container for training a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, train_dl, val_dl):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"Train for a single epoch and return the loss.\"\"\"\n",
    "        loss, n = 0, 0\n",
    "        for x, y in self.train_dl:\n",
    "            y_hat = self.model.forward(x)\n",
    "            batch_loss = self.model.criterion.forward(y, y_hat)\n",
    "            self.model.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += len(y) * batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n\n",
    "            \n",
    "    def train(self, n_epochs):\n",
    "        \"\"\"Train for multiple epochs.\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self._train()\n",
    "            self.model.step(self.lr)\n",
    "            print(f\"{epoch= :.2d} | {loss= :.3f}\")\n",
    "#              val_loss, val_metric = self.evaluate(self.val_dl)\n",
    "#             print(f\"{epoch= :2d} | {loss= :.3f} | {val_loss= :.3f} | {val_metric= :.3f}\")\n",
    "    \n",
    "#     def evaluate(self, dl):\n",
    "#         \"\"\"Return loss and metric on validation or test set.\"\"\"\n",
    "#         loss, n, metric = 0, 0, 0\n",
    "#         for x, y in dl:\n",
    "#             y_hat = self.model.forward(dl)\n",
    "#             batch_loss = self.model.criterion.forward(y, y_hat)\n",
    "#             batch_metric = self.model.metric(y, y_hat)\n",
    "#             loss += len(y) * batch_loss\n",
    "#             metric_ += len(y) * batch_metric\n",
    "#             n += len(y)\n",
    "#         return loss / n, metric / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data\n",
    "\n",
    "We'll use sklearn's [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) for our binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:17:30.234099Z",
     "start_time": "2020-07-04T03:17:30.208642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:17:30.647588Z",
     "start_time": "2020-07-04T03:17:30.639074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test-split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:17:31.212204Z",
     "start_time": "2020-07-04T03:17:31.206485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "mu, sigma = X_train.mean(), X_train.std()\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_val = (X_val - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:17:54.424264Z",
     "start_time": "2020-07-04T03:17:54.419716Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularData(Dataset):\n",
    "    \"\"\"Container for tabular data.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:18:18.497793Z",
     "start_time": "2020-07-04T03:18:18.492094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_ds = TabularData(X_train, y_train)\n",
    "val_ds = TabularData(X_val, y_val)\n",
    "\n",
    "batch_size=50\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Now we're ready to put our model to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:23:08.513074Z",
     "start_time": "2020-07-04T03:23:08.507316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# Initialise model\n",
    "model = OneLayerBinaryClassifier(n_inp)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "metric = accuracy\n",
    "optimizer = SGD(model, lr=0.1)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:23:08.825781Z",
     "start_time": "2020-07-04T03:23:08.794248Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,) doesn't match the broadcast shape (569,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a659739fe524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-9f18f6fc9fe6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m\"\"\"Train for multiple epochs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{epoch= :.2d} | {loss= :.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9f18f6fc9fe6>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-19259e7cb342>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"\"\"Update weights and biases of linear layers.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#self.model.linear.weights -= self.lr * self.model.linear.grad_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,) doesn't match the broadcast shape (569,)"
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
