{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Neural-Nets-Are-Not-Black-Boxes\" data-toc-modified-id=\"Neural-Nets-Are-Not-Black-Boxes-1\">Neural Nets Are Not Black Boxes</a></span></li><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-2\">Objective</a></span></li><li><span><a href=\"#Back-propagation-(a.k.a-The-Chain-Rule)\" data-toc-modified-id=\"Back-propagation-(a.k.a-The-Chain-Rule)-3\">Back-propagation (a.k.a The Chain Rule)</a></span></li><li><span><a href=\"#(Binary)-Cross-Entropy-Loss\" data-toc-modified-id=\"(Binary)-Cross-Entropy-Loss-4\">(Binary) Cross Entropy Loss</a></span></li><li><span><a href=\"#Activations\" data-toc-modified-id=\"Activations-5\">Activations</a></span></li><li><span><a href=\"#Linear-Layer\" data-toc-modified-id=\"Linear-Layer-6\">Linear Layer</a></span></li><li><span><a href=\"#Putting-It-All-Together\" data-toc-modified-id=\"Putting-It-All-Together-7\">Putting It All Together</a></span></li><li><span><a href=\"#Our-Evaluation-Metric\" data-toc-modified-id=\"Our-Evaluation-Metric-8\">Our Evaluation Metric</a></span></li><li><span><a href=\"#Trainer\" data-toc-modified-id=\"Trainer-9\">Trainer</a></span></li><li><span><a href=\"#Pre-process-Data\" data-toc-modified-id=\"Pre-process-Data-10\">Pre-process Data</a></span></li><li><span><a href=\"#Datasets-&amp;-DataLoaders\" data-toc-modified-id=\"Datasets-&amp;-DataLoaders-11\">Datasets &amp; DataLoaders</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-12\">Train</a></span></li><li><span><a href=\"#Comparison-with-sklearn\" data-toc-modified-id=\"Comparison-with-sklearn-13\">Comparison with sklearn</a></span></li><li><span><a href=\"#Adding-a-Second-Layer\" data-toc-modified-id=\"Adding-a-Second-Layer-14\">Adding a Second Layer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets Are Not Black Boxes \n",
    "\n",
    "> \"If you think neural networks are black boxes, you're not alone. Let's dismantle this misconception piece-by-piece as we build our own network from scratch, starting with logistic regression.\"\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [deep learning]\n",
    "- images: images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.889529Z",
     "start_time": "2020-07-09T03:13:32.435690Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer, load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.894561Z",
     "start_time": "2020-07-09T03:13:32.892335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "The goal of this notebook is to build a simple binary logistic classifier as a neural network. The network will consist of a single linear layer followed be a sigmoid activation and binary cross entropy as the loss function. When we're done we'll see that we're able to identify malignant tumors on sklearn's breast cancer dataset with 90% accuracy in just 10 epochs. \n",
    "\n",
    "We'll begin by deriving the back-propagation equations by hand for our particular scenario. In the process we'll realize that what we've done will generalize immediately to networks with an arbitrary number of layers, meaning we'll have built a framework that can model __any__ feedforward neural network. All by starting with binary logistic classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-propagation (a.k.a The Chain Rule)\n",
    "\n",
    "We can view our logistic regression network as the composition of three functions\n",
    "\n",
    "$$x \\to \\text{BCE} \\circ \\text{Sigmoid} \\circ \\text{Linear}(x)$$\n",
    "\n",
    "While the loss function is not usually viewed as a layer of the network, treating it as the final layer makes computing the gradients easier. Let's denote the output of the $i$-th layer by $x_i$ so that\n",
    "\n",
    "\\begin{align}\n",
    "    x_1 &= \\text{Linear}(x)     \\\\\n",
    "    x_2 &= \\text{Sigmoid}(x_1)  \\\\\n",
    "    x_3 &= \\text{BCE}(x_2)\n",
    "\\end{align}\n",
    "\n",
    "Since the last output we have is $x_3 = \\text{BCE}(x_2)$, the first gradient we have to compute is the gradient of $\\text{BCE}$ with respect to $x_2$\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_2} = \\frac{\\partial \\text{BCE}}{\\partial x_2}(x_2) $$\n",
    "\n",
    "Next, we have $x_2 = \\text{Sigmoid}(x_1)$. The chain rule tells us\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_1} = \\frac{\\partial \\text{BCE}}{\\partial x_2} \\times \\frac{\\partial \\text{Sigmoid}}{\\partial x_1}(x_1)$$\n",
    "\n",
    "Last we have $x_1 = \\text{Linear}(x)$, so the final gradient is\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x} = \\frac{\\partial \\text{BCE}}{\\partial x_1} \\times \\frac{\\partial \\text{Linear}}{\\partial x_1}(x)$$\n",
    "\n",
    "Notice something peculiar? The first gradient we computed--the gradient with respect to the Sigmoid layer--is used to compute the next gradient--the gradient with respect to the preceding Linear layer. To compute the gradients of any network, we simply start at the final layer and successively pass the gradient back to the previous layer. That's why it's called backpropagation and not \"just the chain rule\". It really is helpful to envision passing the gradients backwards through the network like a baton.\n",
    "\n",
    "__Note:__ So far we've been treating the input $x$ as a single variable, but most of the time $x$ will have more than one dimension. Don't worry, computing the gradients in the multi-variate case is more or less the same (though it involves something called the Jacobian--but we'll pretend we didn't hear that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Binary) Cross Entropy Loss\n",
    "\n",
    "The only difference between binary cross entropy and cross entropy is that cross entropy requires an output probability for every single class, whereas binary cross entropy requires just a single output probability--the probability of the positive class (usually). This is slightly annoying, because it means their derivatives have different functional forms. We'll stick with binary cross entropy.\n",
    "\n",
    "Binary cross entropy penalizes predictions by the logarithm of their probabilities $\\hat{y}$ (which we called $x_2$ above):\n",
    "\n",
    "$$\\text{BCE}(y, \\hat{y}) = -[y \\ln(\\hat{y}) + (1 - y)\\ln(1 - \\hat{y})]$$\n",
    "\n",
    "After simplifying, you'll find its derivative is\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})}$$\n",
    "\n",
    "Notice that we'll need to cache the labels $y$ and probabilities $\\hat{y}$ to compute the backward pass. Since this is the first gradient we compute, there's no previous gradient to take in. However, the next gradient (belonging to the Sigmoid layer) will receive the above BCE gradient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.901527Z",
     "start_time": "2020-07-09T03:13:32.896799Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    \"\"\"Container for the forward and backward pass of BCE.\"\"\"\n",
    "    \n",
    "    def forward(self, y, y_hat):\n",
    "        \"\"\"Return binary cross entropy given targets and predictions.\"\"\"\n",
    "        self.y, self.y_hat, = y, y_hat.clip(min=1e-8, max=1-1e-8)\n",
    "        return np.where(y==1, -np.log(self.y_hat), -np.log(1 - self.y_hat))\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate the gradient with respect to soft predictions.\"\"\"\n",
    "        return (self.y_hat - self.y) / (self.y_hat * (1 - self.y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations\n",
    "\n",
    "The easiest components to handle are the activation functions. Our activation is Sigmoid, which you'll often see defined as one of \n",
    "\n",
    "$$\\text{Sigmoid}(x) = \\frac{1}{1 + \\text{exp}(-x)} \\quad \\text{or} \\quad \\frac{\\text{exp}(x)}{1 + \\text{exp}(x)}$$\n",
    "\n",
    "It turns out that we need both versions to implement a numerically stable version of Sigmoid. Notice how when $x$ is very negative, $\\text{exp}(-x)$ is incredibly large, and when $x$ is very positive $\\text{exp}(x)$ is incredibly large--in both cases, too large to store in memory. The easy fix is to use the first version when $x > 0$ and the second when $x < 0$.\n",
    "\n",
    "After simplifying, you'll find the derivative of $\\text{Sigmoid}$ is\n",
    "\n",
    "$$\\frac{\\partial \\text{Sigmoid}}{\\partial x} = \\text{Sigmoid}(x)(1 - \\text{Sigmoid}(x))$$\n",
    "\n",
    "You may have noticed something peculiar when comparing this derivative to that of BCE. Since we denoted $\\hat{y}$ to be the output of the Sigmoid layer, the derivative of sigmoid is exactly the same as the denominator in the derivative of BCE, so the two terms will cancel when multiplied (which is exactly what the chain rule tells us will happen). This is the reason you see functions like PyTorch's `binary_cross_entropy_with_logits`, which skips the Sigmoid activation and computes BCE directly from the inputs to the Sigmoid layer (which people call logits). It's more efficient computationally to do both in one go. However, since we're just trying to get our hands dirty to understand how networks work, we won't worry about optimizing things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.907926Z",
     "start_time": "2020-07-09T03:13:32.903589Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Container for the forward and backward pass of sigmoid.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a sigmoid layer.\"\"\"\n",
    "        self.y_hat = np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return self.y_hat\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Backpropagate the gradient given the previous gradient.\"\"\"\n",
    "        return self.y_hat * (1 - self.y_hat) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer\n",
    "\n",
    "The last component we need to implement is the linear layer, which contains weights and biases. Denoting the output of this layer by $z$, we have\n",
    "\n",
    "$$z = \\text{Linear}(x) = xw + b$$\n",
    "\n",
    "If $x$ is a mini-batch of shape $(bs, n_{inp})$ then $w$ has shape $(n_{inp}, 1)$ and $b$ has shape $(1,)$, with addition being done via broadcasting. When computing the gradient, we'll imagine that we just have a batch size of one \n",
    "\n",
    "$$x = [x_1, \\dots, x_{n_{inp}}]$$ \n",
    "\n",
    "There are two gradients to compute this time around, one with respect to the weights and the other with respect to the bias. To make life easier, let's write things out in terms of coordinates:\n",
    "\n",
    "$$z_i = \\sum_{k=1}^{n_{inp}} x_k w_{ki} + b$$\n",
    "\n",
    "Then we get\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial w_{ki}} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}\n",
    "{\\partial w_{ki}} = x_k \\frac{\\partial \\text{BCE}}{\\partial z_i}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial b} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}{\\partial b} = \\frac{\\partial \\text{BCE}}{\\partial z_i}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we'll already have the gradient of the loss with respect to the output $z$ of the linear layer stored in a variable $\\text{grad}$ when it's time to compute the gradients with respect to the weights and biases. The nice thing is that $\\text{grad}$ is exactly the gradient with respect to the $b$, so we only need to figure out how to write the gradient with respect to $w$ as a matrix product.\n",
    "\n",
    "Whenever I have a hard time doing something like this, I just focus on getting the shapes right:\n",
    "\n",
    "\\begin{align}\n",
    "  &\\bullet x \\text{ has shape } (bs, n_{inp}) \\\\\n",
    "  &\\bullet \\text{grad has shape } (bs, 1) \\\\\n",
    "  &\\bullet \\text{grad}_w \\text{ has shape } (n_{inp}, 1)\n",
    "\\end{align}\n",
    "\n",
    "The only way we can multiply $x$ and $\\text{grad}$ and get something of shape $(n_{inp}, 1)$ is to resize $x$ to have shape $(bs, n_{inp}, 1)$ and $\\text{grad}$ to have shape $(bs, 1, 1)$ so that ordinary matrix multiplication over the last two dimensions gives the shape $(n_{inp}, 1)$. The last thing to remember is that we average the gradients over the batch dimension to produce the final gradient updates.\n",
    "\n",
    "Note that were there another linear layer we would also need to compute the gradient of the loss with respect to the inputs $x$ so that we could keep backpropagating the gradient, however, this isn't anymore complicated than what we've done so far, so let's do it anyways!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\text{BCE}}{x_k} = \\sum_{k=1}^{n_{inp}} \\frac{\\partial \\text{BCE}}{\\partial z_k} \\times \\frac{\\partial z_k}{\\partial x_k} = \\sum_{k=1}^{n_{inp}} \\frac{\\partial \\text{BCE}}{\\partial z_k} w_{ki} = \\text{grad} \\times w^t$$\n",
    "\n",
    "As always, let's do a sanity check on the dimensions involved to make sure nothing has gone horribly wrong. Since $\\text{grad}$ has shape $(bs, 1)$ and $w$ has shape $(n_{inp}, 1)$, $\\text{grad} \\times w^t$ has shape $(bs, n_{inp})$, which is exactly the shape of $x$--just as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.915093Z",
     "start_time": "2020-07-09T03:13:32.909581Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Container for the forward and backward pass of a linear layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inp, n_out):\n",
    "        k = 1 / np.sqrt(n_inp)\n",
    "        self.weights = np.random.uniform(-k, k, (n_inp, n_out))\n",
    "        self.bias = np.zeros(n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a linear layer.\"\"\"\n",
    "        self.x = x\n",
    "        return x @ self.weights + self.bias\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Compute the gradients of the weights and biases given previous gradient.\"\"\"\n",
    "        self.grad_w = (self.x[:,:,None] @ grad[:,None,:]).mean(axis=0)\n",
    "        self.grad_b = grad.mean(axis=0)\n",
    "        return grad @ self.weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "\n",
    "Wohoo! It's finally time to string together all the work we've done so far into a single network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.921028Z",
     "start_time": "2020-07-09T03:13:32.916602Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"Container for a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, criterion, metric):\n",
    "        \"\"\"Initialise layers, loss criterion, and evaluation metric.\"\"\"\n",
    "        self.layers = layers\n",
    "        self.criterion = criterion\n",
    "        self.metric = metric\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through the network.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate gradients to the start of the network.\"\"\"\n",
    "        grad = self.criterion.backward()\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.926928Z",
     "start_time": "2020-07-09T03:13:32.923228Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Container for updating a model's weights via SGD.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "                  \n",
    "    def step(self):\n",
    "        \"\"\"Update the weights and biases of all linear layers.\"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.weights -= self.lr * layer.grad_w\n",
    "                layer.bias -= self.lr * layer.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Evaluation Metric\n",
    "\n",
    "For simplicity, we'll just consider accuracy for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.933070Z",
     "start_time": "2020-07-09T03:13:32.930396Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    \"\"\"Compute accuracy given soft binary predictions.\"\"\"\n",
    "    y_pred = y_hat > 0.5\n",
    "    return (y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "To make life easier, let's wrap all of the functionality we need to train a network into a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.944259Z",
     "start_time": "2020-07-09T03:13:32.935653Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Container for training a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, train_dl, val_dl):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"Train for a single epoch and return the loss.\"\"\"\n",
    "        loss, n = 0, 0\n",
    "        for x, y in self.train_dl:\n",
    "            y_hat = self.model.forward(x)\n",
    "            batch_loss = self.model.criterion.forward(y, y_hat).sum()\n",
    "            self.model.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n\n",
    "            \n",
    "    def train(self, n_epochs, log_level=1):\n",
    "        \"\"\"Train for multiple epochs.\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self._train()\n",
    "            self.optimizer.step()\n",
    "            val_loss, val_metric = self.evaluate(self.val_dl)\n",
    "            if (epoch + 1) % log_level == 0:\n",
    "                print(f\"{epoch= :2d} | {loss= :.3f} | {val_loss= :.3f} | {val_metric= :.3f}\")\n",
    "    \n",
    "    def evaluate(self, dl):\n",
    "        \"\"\"Return loss and metric on validation or test set.\"\"\"\n",
    "        loss, n, metric = 0, 0, 0\n",
    "        for x, y in dl:\n",
    "            y_hat = self.model.forward(x)\n",
    "            batch_loss = self.model.criterion.forward(y, y_hat).sum()\n",
    "            batch_metric = self.model.metric(y, y_hat)\n",
    "            metric += len(y) * batch_metric\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n, metric / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data\n",
    "\n",
    "We'll use sklearn's [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) for our binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.962581Z",
     "start_time": "2020-07-09T03:13:32.946594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.968568Z",
     "start_time": "2020-07-09T03:13:32.963958Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test-split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.973776Z",
     "start_time": "2020-07-09T03:13:32.970426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "mu, sigma = X_train.mean(), X_train.std()\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_val = (X_val - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.980167Z",
     "start_time": "2020-07-09T03:13:32.975311Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Container for returning inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1, 1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __setitem__(self, idx, val):\n",
    "        self.X[idx], self.y[idx] = val\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.986974Z",
     "start_time": "2020-07-09T03:13:32.981608Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Container for returning a mini-batch of inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, ds, batch_size, shuffle=False):\n",
    "        self.ds = ds\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        \"\"\"Shuffle inputs and targets.\"\"\"\n",
    "        idxs = np.random.permutation(len(self.ds))\n",
    "        self.ds = Dataset(*self.ds[idxs])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a mini-batch of inputs and targets.\"\"\"\n",
    "        if self.shuffle: self.shuffle_data()\n",
    "        n_batches = len(self.ds) // self.batch_size\n",
    "        for i in range(n_batches):\n",
    "            yield self.ds[i * self.batch_size: (i + 1) * self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.992810Z",
     "start_time": "2020-07-09T03:13:32.989077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_ds = Dataset(X_train, y_train)\n",
    "val_ds = Dataset(X_val, y_val)\n",
    "\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(X_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Now we're ready to put our model to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:32.998371Z",
     "start_time": "2020-07-09T03:13:32.994748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion, metric)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.1)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:33.017273Z",
     "start_time": "2020-07-09T03:13:33.000010Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.664 | val_loss= 0.626 | val_metric= 0.702\n",
      "epoch=  1 | loss= 0.601 | val_loss= 0.548 | val_metric= 0.860\n",
      "epoch=  2 | loss= 0.534 | val_loss= 0.495 | val_metric= 0.877\n",
      "epoch=  3 | loss= 0.493 | val_loss= 0.462 | val_metric= 0.895\n",
      "epoch=  4 | loss= 0.462 | val_loss= 0.430 | val_metric= 0.868\n",
      "epoch=  5 | loss= 0.436 | val_loss= 0.400 | val_metric= 0.904\n",
      "epoch=  6 | loss= 0.417 | val_loss= 0.381 | val_metric= 0.904\n",
      "epoch=  7 | loss= 0.394 | val_loss= 0.363 | val_metric= 0.895\n",
      "epoch=  8 | loss= 0.378 | val_loss= 0.357 | val_metric= 0.895\n",
      "epoch=  9 | loss= 0.366 | val_loss= 0.339 | val_metric= 0.904\n"
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with sklearn\n",
    "\n",
    "Let's compare how our logistic network stacks up against sklearn's own logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:33.034131Z",
     "start_time": "2020-07-09T03:13:33.018912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122807017543859"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're close!\n",
    "sklearn_model = LogisticRegression(random_state=seed)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a Second Layer\n",
    "\n",
    "Let's add another linear layer to our network, followed, of course, by an activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:13:33.041638Z",
     "start_time": "2020-07-09T03:13:33.036173Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"Container for the forward and backward pass of ReLU.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through ReLU.\"\"\"\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Return the gradient where x is positive, otherwise zero.\"\"\"\n",
    "        return np.where(self.x > 0, grad, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:16:46.830358Z",
     "start_time": "2020-07-09T03:16:46.823405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 20), ReLU(), Linear(20, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion, accuracy)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.10)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T03:16:47.051136Z",
     "start_time": "2020-07-09T03:16:46.993343Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.664 | val_loss= 0.631 | val_metric= 0.886\n",
      "epoch=  1 | loss= 0.621 | val_loss= 0.587 | val_metric= 0.921\n",
      "epoch=  2 | loss= 0.579 | val_loss= 0.544 | val_metric= 0.895\n",
      "epoch=  3 | loss= 0.539 | val_loss= 0.501 | val_metric= 0.895\n",
      "epoch=  4 | loss= 0.501 | val_loss= 0.461 | val_metric= 0.895\n",
      "epoch=  5 | loss= 0.464 | val_loss= 0.426 | val_metric= 0.877\n",
      "epoch=  6 | loss= 0.427 | val_loss= 0.390 | val_metric= 0.904\n",
      "epoch=  7 | loss= 0.400 | val_loss= 0.363 | val_metric= 0.895\n",
      "epoch=  8 | loss= 0.370 | val_loss= 0.335 | val_metric= 0.895\n",
      "epoch=  9 | loss= 0.346 | val_loss= 0.316 | val_metric= 0.895\n",
      "epoch= 10 | loss= 0.328 | val_loss= 0.297 | val_metric= 0.904\n",
      "epoch= 11 | loss= 0.309 | val_loss= 0.281 | val_metric= 0.904\n",
      "epoch= 12 | loss= 0.293 | val_loss= 0.268 | val_metric= 0.904\n",
      "epoch= 13 | loss= 0.277 | val_loss= 0.257 | val_metric= 0.904\n",
      "epoch= 14 | loss= 0.269 | val_loss= 0.249 | val_metric= 0.895\n",
      "epoch= 15 | loss= 0.266 | val_loss= 0.247 | val_metric= 0.895\n",
      "epoch= 16 | loss= 0.252 | val_loss= 0.234 | val_metric= 0.904\n",
      "epoch= 17 | loss= 0.246 | val_loss= 0.233 | val_metric= 0.895\n",
      "epoch= 18 | loss= 0.239 | val_loss= 0.228 | val_metric= 0.895\n",
      "epoch= 19 | loss= 0.234 | val_loss= 0.218 | val_metric= 0.904\n"
     ]
    }
   ],
   "source": [
    "trainer.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
