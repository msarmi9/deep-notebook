{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Neural-Nets-Are-Not-Black-Boxes\" data-toc-modified-id=\"Neural-Nets-Are-Not-Black-Boxes-1\">Neural Nets Are Not Black Boxes</a></span></li><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-2\">Objective</a></span></li><li><span><a href=\"#Back-propagation-is-the-Chain-Rule\" data-toc-modified-id=\"Back-propagation-is-the-Chain-Rule-3\">Back-propagation is the Chain Rule</a></span></li><li><span><a href=\"#Binary-Cross-Entropy\" data-toc-modified-id=\"Binary-Cross-Entropy-4\">Binary Cross Entropy</a></span></li><li><span><a href=\"#Activations\" data-toc-modified-id=\"Activations-5\">Activations</a></span></li><li><span><a href=\"#Linear-Layer\" data-toc-modified-id=\"Linear-Layer-6\">Linear Layer</a></span></li><li><span><a href=\"#Putting-It-All-Together\" data-toc-modified-id=\"Putting-It-All-Together-7\">Putting It All Together</a></span></li><li><span><a href=\"#Our-Evaluation-Metric\" data-toc-modified-id=\"Our-Evaluation-Metric-8\">Our Evaluation Metric</a></span></li><li><span><a href=\"#Trainer\" data-toc-modified-id=\"Trainer-9\">Trainer</a></span></li><li><span><a href=\"#Pre-process-Data\" data-toc-modified-id=\"Pre-process-Data-10\">Pre-process Data</a></span></li><li><span><a href=\"#Datasets-&amp;-DataLoaders\" data-toc-modified-id=\"Datasets-&amp;-DataLoaders-11\">Datasets &amp; DataLoaders</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-12\">Train</a></span></li><li><span><a href=\"#Comparison-with-sklearn\" data-toc-modified-id=\"Comparison-with-sklearn-13\">Comparison with sklearn</a></span></li><li><span><a href=\"#Adding-a-Second-Layer\" data-toc-modified-id=\"Adding-a-Second-Layer-14\">Adding a Second Layer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets Are Not Black Boxes \n",
    "\n",
    "If you think neural nets are black boxes, you're certainly not alone. While they may not be as interpretable as something like a random forest (at least not yet), neural nets are in no way black boxes. In this post we'll break down this misconception piece by piece as we build our own network from scratch, starting with logistic regression.    \n",
    "\n",
    "This post is very much inspired by [this fantastic post](https://sgugger.github.io/a-simple-neural-net-in-numpy.html#a-simple-neural-net-in-numpy) by Sylvain Gugger. We won't pretend to add to or improve upon Sylvain's post; we just want to explain things in our own way to help us understand things a little bit more clearly. This will be the first of a series of posts in which we'll write our own DNN, CNN, and RNN from scratch. You can find the source code for all of these posts at our [tinytorch repo](https://github.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.001670Z",
     "start_time": "2020-08-02T17:18:33.563221Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.006504Z",
     "start_time": "2020-08-02T17:18:36.004034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "Our goal is to construct a binary logistic classifier as a neural network. The network will consist of a single linear layer followed by a sigmoid activation with binary cross entropy as the loss function. We'll begin by deriving the back-prop equations for our particular scenario and in the process we'll realize that what we've done generalizes immediately to networks with any number of layers and arbitrary activations. In other words, we'll have developed a framework that can model any feedforward network--all by starting with ordinary logistic regression. \n",
    "\n",
    "Actually, this isn't all that surprising when you think about it, since logistic regression is a linear layer followed by sigmoid and feedforward networks are just a bunch of linear layers stacked together with non-linear activations in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-propagation is the Chain Rule\n",
    "\n",
    "Back-propagation is nothing more than the chain rule. We can view our logistic network as the composition of three functions\n",
    "\n",
    "$$x \\to \\text{BCE} \\circ \\text{Sigmoid} \\circ \\text{Linear}(x)$$\n",
    "\n",
    "While the loss function is not usually viewed as a layer of the network, treating it as the final layer makes computing the gradients easier. Let's denote the output of the $i$-th layer by $x_i$ so that\n",
    "\n",
    "\\begin{align}\n",
    "    x_1 &= \\text{Linear}(x)     \\\\\n",
    "    x_2 &= \\text{Sigmoid}(x_1)  \\\\\n",
    "    x_3 &= \\text{BCE}(x_2)\n",
    "\\end{align}\n",
    "\n",
    "The first gradient we have to compute is the gradient of $\\text{BCE}$ with respect to the activations $x_2$\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_2} = \\frac{\\partial \\text{BCE}}{\\partial x_2}(x_2) $$\n",
    "\n",
    "Next we have to compute the gradient with respect to the linear outputs $x_1$. The chain rule tells us\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_1} = \\frac{\\partial \\text{BCE}}{\\partial x_2} \\times \\frac{\\partial \\text{Sigmoid}}{\\partial x_1}(x_1)$$\n",
    "\n",
    "Last, we'll need to compute the gradient with respect to the original inputs $x$\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x} = \\frac{\\partial \\text{BCE}}{\\partial x_1} \\times \\frac{\\partial \\text{Linear}}{\\partial x_1}(x)$$\n",
    "\n",
    "See the pattern? The first gradient we computed--the gradient with respect to the network's final activations--is used to compute the following gradient--the gradient with respect to the linear outputs--which are in turn used to compute the gradient with respect to the original inputs. To compute the gradients of any network, we simply start at the last layer and successively pass the gradients backwards to the preceding layer until we arrive at our original inputs. That's why it's called back-propagation and not \"just the chain rule\". It really is helpful to picture passing the gradients backwards through the network like a baton.\n",
    "\n",
    "We'll compute each of these gradients in turn, starting with the final layer and working our way backwards to the original inputs.\n",
    "\n",
    "__Note:__ So far we've been treating the input $x$ as a single variable, but most of the time $x$ will have more than one dimension. Computing the gradients in the multi-variate case is more or less the same (although it involves something called the Jacobian, but we'll pretend we didn't hear that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Cross Entropy\n",
    "\n",
    "Binary cross entropy penalizes predictions by the logarithm of their confidence. Given labels $y$ which are either zero or one and probabilities $\\hat{y}$ for the positive class, we add $-\\ln(\\text{P}(y=1))$ to the loss whenever $y = 1$ and $-\\ln(\\text{P}(y=0))$ whenever $y = 0$. In other words,\n",
    "\n",
    "$$\\text{BCE}(\\hat{y}, y) = -[y \\ln(\\hat{y}) + (1 - y)\\ln(1 - \\hat{y})]$$\n",
    "\n",
    "After simplifying, you'll find its derivative is\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})}$$\n",
    "\n",
    "To avoid potential division-by-zero errors, we'll clip the output probabilities $\\hat{y}$ so that they're not too close to either zero or one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T18:54:35.909848Z",
     "start_time": "2020-08-02T18:54:35.902750Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    \"\"\"Container for the forward and backward pass of BCE.\"\"\"\n",
    "    \n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"Return binary cross entropy given predictions and targets.\"\"\"\n",
    "        self.y_hat, self.y = y_hat.clip(min=1e-8, max=1-1e-8), y\n",
    "        return -np.where(y==1, np.log(self.y_hat), np.log(1 - self.y_hat))\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate the gradient with respect to predictions.\"\"\"\n",
    "        return (self.y_hat - self.y) / (self.y_hat * (1 - self.y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations\n",
    "\n",
    "The easiest components to handle are the activation functions. Our activation is sigmoid, which you'll often see defined as one of \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)} \\quad \\text{or} \\quad \\frac{\\text{exp}(x)}{1 + \\text{exp}(x)}$$\n",
    "\n",
    "It turns out that we need both versions to implement a numerically stable version of sigmoid. Why? Notice how when $x$ is very negative, $\\text{exp}(-x)$ is very large, and when $x$ is very positive, $\\text{exp}(x)$ is very large--in both cases too large to store in memory. The easy fix is to use the first version when $x > 0$ and the second when $x < 0$.\n",
    "\n",
    "After simplifying, you'll find the derivative of sigmoid is\n",
    "\n",
    "$$\\frac{\\partial \\sigma}{\\partial x} = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "Notice something interesting? Since the probabilities $\\hat{y}$ are the outputs $\\sigma(x)$ of sigmoid, this is exactly the denominator of the BCE gradient we computed above, meaning the two terms will cancel when we compute the gradient of BCE with respect to the outputs $x_1$ of our network's final (and only) linear layer\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_1} = \\hat{y} - y $$\n",
    "\n",
    "In other words, the gradient of the loss with respect to the network's final linear outputs is just the difference between the probabilities $\\hat{y}$ and the labels $y$. The further apart they are (i.e. the worse our predictions are), the larger the gradient and the larger the update to the last linear layer's weights in the SGD step (remember, the chain rule tells us that the gradient with respect to the final linear outputs appear as a factor in the gradient with respect to the last linear layer's weights themselves). \n",
    "\n",
    "This is terrific because it means the weights of our network will change gradually as we train and won't spike or drop suddenly, which would be the case if the gradients were a quadratic or higher-order function of the prediction errors. It also demonstrates nicely how a network adjusts its weights based on the error of its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.022252Z",
     "start_time": "2020-08-02T17:18:36.017475Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Container for the forward and backward pass of sigmoid.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a sigmoid layer.\"\"\"\n",
    "        self.y_hat = np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return self.y_hat\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Backpropagate the gradient given the preceding gradient.\"\"\"\n",
    "        return self.y_hat * (1 - self.y_hat) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer\n",
    "\n",
    "The last and most difficult component we need to implement is the linear layer, which contains weights and biases. Denoting the output of this layer by $z$, we have\n",
    "\n",
    "$$z = xw + b$$\n",
    "\n",
    "If $x$ is a mini-batch of shape $(bs, n_{inp})$ then $w$ has shape $(n_{inp}, 1)$ and $b$ has shape $(1,)$, with addition being done via broadcasting. To make things easier, for the moment let's just imagine we have a batch size of one:\n",
    "\n",
    "$$x = [x_1, \\dots, x_{n_{inp}}]$$ \n",
    "\n",
    "There are two gradients to compute this time around, one with respect to the weights and another with respect to the bias. To make life easier, let's everything out in terms of coordinates:\n",
    "\n",
    "$$z_i = \\sum_{k=1}^{n_{inp}} x_k w_{ki} + b$$\n",
    "\n",
    "Then we get\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial w_{ki}} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}\n",
    "{\\partial w_{ki}} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times x_k$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial b} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}{\\partial b} = \\frac{\\partial \\text{BCE}}{\\partial z_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we'll already have the gradient with respect to the output $z$ of the linear layer stored beforehand (say in a variable called $\\text{grad}$) when it's time to compute the gradient with respect to the weights and biases. Notice something really nice? $\\text{grad}$ is exactly the gradient with respect to $b$, leaving only the gradient with respect to the weights to compute. The main thing we need to do is figure out how to write this as a matrix product.\n",
    "\n",
    "Whenever I have a hard time doing something like this, I just focus on getting the shapes right:\n",
    "\n",
    "\\begin{align}\n",
    "  &\\bullet x \\text{ has shape } (bs, n_{inp}) \\\\\n",
    "  &\\bullet \\text{grad has shape } (bs, 1) \\\\\n",
    "  &\\bullet \\text{grad}_w \\text{ has shape } (n_{inp}, 1)\n",
    "\\end{align}\n",
    "\n",
    "The only way we can multiply $x$ and $\\text{grad}$ and get something of shape $(n_{inp}, 1)$ is to resize $x$ to have shape $(bs, n_{inp}, 1)$ and $\\text{grad}$ to have shape $(bs, 1, 1)$ so that ordinary matrix multiplication over the last two dimensions gives the shape $(n_{inp}, 1)$.\n",
    "\n",
    "Note that were there another linear layer we would also need to compute the gradient of the loss with respect to the inputs $x$ so that we could keep back-propagating the gradients. This isn't anymore complicated than what we've done so far and doing so will allow us to build networks with any number of layers, so let's go ahead and do it! Since $x_k$ appears in each of the activations $z_i$, the gradient will respect to $x_k$ will involve summing up all of the intermediate gradients with respect to $z_i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\text{BCE}}{x_k} = \\sum_{i=1}^{n_{inp}} \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}{\\partial x_k} = \\sum_{i=1}^{n_{inp}} \\frac{\\partial \\text{BCE}}{\\partial z_i} w_{ki}$$\n",
    "\n",
    "Re-writing this as a matrix product, we get\n",
    "\n",
    "$$ \\frac{\\partial \\text{BCE}}{x_k} = \\text{grad} \\times W^t$$\n",
    "\n",
    "where $W$ is the weight matrix. Let's do a sanity check on the dimensions involved to make sure nothing has gone horribly wrong. Since $\\text{grad}$ has shape $(bs, 1)$ and $W$ has shape $(n_{inp}, 1)$, $\\text{grad} \\times W^t$ has shape $(bs, n_{inp})$, which is exactly the shape of $x$--just as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.030456Z",
     "start_time": "2020-08-02T17:18:36.024717Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Container for the forward and backward pass of a linear layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inp, n_out):\n",
    "        k = 1 / np.sqrt(n_inp)\n",
    "        self.weights = np.random.uniform(-k, k, (n_inp, n_out))\n",
    "        self.bias = np.zeros(n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a linear layer.\"\"\"\n",
    "        self.x = x\n",
    "        return x @ self.weights + self.bias\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Compute the gradients of the weights and biases given previous gradient.\"\"\"\n",
    "        self.grad_w = (self.x[:,:,None] @ grad[:,None,:]).mean(axis=0)\n",
    "        self.grad_b = grad.mean(axis=0)\n",
    "        return grad @ self.weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "\n",
    "Wohoo! It's finally time to string together all the work we've done so far into a single network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.037732Z",
     "start_time": "2020-08-02T17:18:36.032658Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"Container for a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, criterion, metric):\n",
    "        \"\"\"Initialise layers, loss criterion, and evaluation metric.\"\"\"\n",
    "        self.layers = layers\n",
    "        self.criterion = criterion\n",
    "        self.metric = metric\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through the network.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate gradients to the start of the network.\"\"\"\n",
    "        grad = self.criterion.backward()\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.043985Z",
     "start_time": "2020-08-02T17:18:36.039566Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Container for updating a model's weights via SGD.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "                  \n",
    "    def step(self):\n",
    "        \"\"\"Update the weights and biases of all linear layers.\"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.weights -= self.lr * layer.grad_w\n",
    "                layer.bias -= self.lr * layer.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Evaluation Metric\n",
    "\n",
    "For simplicity, we'll just consider accuracy for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.051375Z",
     "start_time": "2020-08-02T17:18:36.048222Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    \"\"\"Compute accuracy given soft binary predictions.\"\"\"\n",
    "    y_pred = y_hat > 0.5\n",
    "    return (y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Trainer\n",
    "\n",
    "To make life easier, let's wrap all of the functionality we need to train a network into a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.063050Z",
     "start_time": "2020-08-02T17:18:36.054491Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Container for training a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, train_dl, val_dl):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"Train for a single epoch and return the loss.\"\"\"\n",
    "        loss, n = 0, 0\n",
    "        for x, y in self.train_dl:\n",
    "            y_hat = self.model.forward(x)\n",
    "            batch_loss = self.model.criterion.forward(y, y_hat).sum()\n",
    "            self.model.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n\n",
    "            \n",
    "    def train(self, n_epochs, log_level=1):\n",
    "        \"\"\"Train for multiple epochs.\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self._train()\n",
    "            self.optimizer.step()\n",
    "            val_loss, val_metric = self.evaluate(self.val_dl)\n",
    "            if (epoch + 1) % log_level == 0:\n",
    "                print(f\"{epoch= :2d} | {loss= :.3f} | {val_loss= :.3f} | {val_metric= :.3f}\")\n",
    "    \n",
    "    def evaluate(self, dl):\n",
    "        \"\"\"Return loss and metric on validation or test set.\"\"\"\n",
    "        loss, n, metric = 0, 0, 0\n",
    "        for x, y in dl:\n",
    "            y_hat = self.model.forward(x)\n",
    "            batch_loss = self.model.criterion.forward(y, y_hat).sum()\n",
    "            batch_metric = self.model.metric(y, y_hat)\n",
    "            metric += len(y) * batch_metric\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n, metric / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data\n",
    "\n",
    "We'll use sklearn's [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) for our binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.081593Z",
     "start_time": "2020-08-02T17:18:36.064601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.088138Z",
     "start_time": "2020-08-02T17:18:36.083157Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test-split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.094130Z",
     "start_time": "2020-08-02T17:18:36.090044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "mu, sigma = X_train.mean(), X_train.std()\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_val = (X_val - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.103060Z",
     "start_time": "2020-08-02T17:18:36.096366Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Container for returning inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1, 1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __setitem__(self, idx, val):\n",
    "        self.X[idx], self.y[idx] = val\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.111229Z",
     "start_time": "2020-08-02T17:18:36.105444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Container for returning a mini-batch of inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, ds, batch_size, shuffle=False):\n",
    "        self.ds = ds\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        \"\"\"Shuffle inputs and targets.\"\"\"\n",
    "        idxs = np.random.permutation(len(self.ds))\n",
    "        self.ds = Dataset(*self.ds[idxs])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a mini-batch of inputs and targets.\"\"\"\n",
    "        if self.shuffle: self.shuffle_data()\n",
    "        n_batches = len(self.ds) // self.batch_size\n",
    "        for i in range(n_batches):\n",
    "            yield self.ds[i * self.batch_size: (i + 1) * self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.116750Z",
     "start_time": "2020-08-02T17:18:36.113372Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_ds = Dataset(X_train, y_train)\n",
    "val_ds = Dataset(X_val, y_val)\n",
    "\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(X_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Now we're ready to put our model to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:25:56.317938Z",
     "start_time": "2020-08-02T17:25:56.311608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion, metric)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.1)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:26:20.227665Z",
     "start_time": "2020-08-02T17:26:20.201544Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.619 | val_loss= 0.577 | val_metric= 0.798\n",
      "epoch=  1 | loss= 0.559 | val_loss= 0.515 | val_metric= 0.895\n",
      "epoch=  2 | loss= 0.514 | val_loss= 0.472 | val_metric= 0.895\n",
      "epoch=  3 | loss= 0.478 | val_loss= 0.440 | val_metric= 0.904\n",
      "epoch=  4 | loss= 0.449 | val_loss= 0.412 | val_metric= 0.895\n",
      "epoch=  5 | loss= 0.421 | val_loss= 0.390 | val_metric= 0.895\n",
      "epoch=  6 | loss= 0.402 | val_loss= 0.374 | val_metric= 0.904\n",
      "epoch=  7 | loss= 0.388 | val_loss= 0.362 | val_metric= 0.895\n",
      "epoch=  8 | loss= 0.370 | val_loss= 0.343 | val_metric= 0.904\n",
      "epoch=  9 | loss= 0.359 | val_loss= 0.332 | val_metric= 0.895\n"
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with sklearn\n",
    "\n",
    "Let's compare how our logistic network stacks up against sklearn's own logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:18:36.164041Z",
     "start_time": "2020-08-02T17:18:36.148496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122807017543859"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're close!\n",
    "sklearn_model = LogisticRegression(random_state=seed)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a Second Layer\n",
    "\n",
    "Let's add another linear layer to our network, followed, of course, by an activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:44:43.291560Z",
     "start_time": "2020-08-02T17:44:43.285189Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"Container for the forward and backward pass of ReLU.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through ReLU.\"\"\"\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Return the gradient where x is positive, otherwise zero.\"\"\"\n",
    "        return np.where(self.x > 0, grad, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:44:44.498950Z",
     "start_time": "2020-08-02T17:44:44.491940Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 20), ReLU(), Linear(20, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion, accuracy)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.10)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:45:20.539083Z",
     "start_time": "2020-08-02T17:45:20.496969Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.683 | val_loss= 0.654 | val_metric= 0.868\n",
      "epoch=  1 | loss= 0.639 | val_loss= 0.611 | val_metric= 0.851\n",
      "epoch=  2 | loss= 0.597 | val_loss= 0.562 | val_metric= 0.912\n",
      "epoch=  3 | loss= 0.554 | val_loss= 0.515 | val_metric= 0.895\n",
      "epoch=  4 | loss= 0.512 | val_loss= 0.473 | val_metric= 0.868\n",
      "epoch=  5 | loss= 0.468 | val_loss= 0.422 | val_metric= 0.904\n",
      "epoch=  6 | loss= 0.425 | val_loss= 0.382 | val_metric= 0.895\n",
      "epoch=  7 | loss= 0.387 | val_loss= 0.367 | val_metric= 0.904\n",
      "epoch=  8 | loss= 0.364 | val_loss= 0.330 | val_metric= 0.912\n",
      "epoch=  9 | loss= 0.335 | val_loss= 0.303 | val_metric= 0.886\n",
      "epoch= 10 | loss= 0.311 | val_loss= 0.280 | val_metric= 0.904\n",
      "epoch= 11 | loss= 0.292 | val_loss= 0.267 | val_metric= 0.904\n"
     ]
    }
   ],
   "source": [
    "trainer.train(12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
